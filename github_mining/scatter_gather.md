Scatter/map/gather is a good "challenge problem" to assess the flexibility of workflow engines.
The problem is: given `f: () -> Bag[T]`, `g: T -> U`, `h: Bag[U] -> U` (associative), compute `h([g(elem) for elem in f()])`.
The DAG would look like: `f -> g0; f -> g1; ... f -> gn; g0 -> h; g1 -> h; ... gn -> h;`

There are several questions we can ask about a given workflow engine's solution to this challenge problem.

- Does the workflow model `g` as operating on single elements or partitions of elements?
  - This is important for reusability; there might already be a wrapper that operates on single elements, so it can be more easily reused if the "map" part of scatter/map/gather operates on single-elements.
  - Perhaps more importantly, if the workflow engine offers caching, the cache-key of `g` should be a single element, not a partition. If the cache-key is a partition, if any element within the partition changes, the entire cache entry is invalid. If any element is inserted or removed in this partition or a prior one, the entire cache entry is invalid.
- Does the workflow schedule one jobs of `g` for every element or does it batch jobs of `g`?
  - If the output of `f` is large and `g` is relatively fast, then scheduling one job per element creates contention on the scheduler. While the programmer should "see" `g` operating on a single-element, under-the-hood, the workflow engine should probably schedule `g` in a fixed-number of almost-equal batches.
- Does a visualization of the workflow DAG contain multiple copies of `g`?
  - Ideally, the workflow engine would support two visualizations: one showing how the workflow is coded and another showing how it is executed. The coding-visualization should not show multiple copies of `g`, eliminating unnecessary "chart noise". Instead `f` could be shaped like an inverted funnel, and `h` like a funnel, indicating the scatter/gather. This would be more suitable for inclusion in a paper. The execution-visualization may include copies of `g` and `h`, which would be more suitable for performance debugging. One could set the number of partitions to 1 for the sake of plotting the coding-visualization from a workflow engine that only has an execution-visualization. However, one would still have to change the node shapes or bold the arrows.
- Can jobs of `g` be dispatched while the `f` is still in progress? Can jobs of `h` be dispatched while some jobs of `g` are still in progress?
  - `f` may be able to output its first result quickly, but take much longer to get to its last result. Ideally, the system can be working on the first few jobs of `g` while the last few results of `f` are coming in, to maximize parallelism. As in a tree-reduction, `h` is The workflow engine should assume that `h` is associative, so `h([u0, u1, u2, u3]) == h([h([u0, u1]), h([u2, u3])])`. `h([u0, u1])` need not wait for `u2` and `u3` to become available. This only matters for performance-sensitive workloads where the scheduler would become swamped by the number of `f` outputs and overhead of dispatching `g`.

- Snakemake has two possible implementations of scatter/gather:
  - Literal [`scatter`/`gather` functions](https://snakemake.readthedocs.io/en/v8.11.6/snakefiles/rules.html#defining-scatter-gather-processes) partition a list into a fixed number of almost-equal partitions. `f`'s between-invocations cache is invalidated if **any** item in the partition was modified or if any item was inserted or deleted before the start of a given partition. `g` can be launched in as soon as a batch is ready, but `h` has to wait for all of them to complete because there is no way to indicate to Snakemake that `h` is a reduction.
  - Specify the `input` dynamically (formerly `dyanamic`, now [`checkpoint`](https://snakemake.readthedocs.io/en/v8.11.6/snakefiles/rules.html#data-dependent-conditional-execution)). `g` can be launched in as soon as a batch is ready, but `h` has to wait for all of them to complete because there is no way to indicate to Snakemake that `h` is a reduction.
- Parsl: doesn't refer to scatter/gather by name, but it can implement them with [`join_app`s](https://parsl.readthedocs.io/en/stable/userguide/joins.html): `join_app_g_h(f())` where `join_app_g_h = parsl.join_app(lambda lst: h(*[g(elem) for elem in lst]))`. Parsl's [between-invocation caching](https://parsl.readthedocs.io/en/stable/userguide/checkpoints.html) would probably work on `g`. Parsl cannot however stream jobs of `g` and jobs of `h`.
- Dask: `f().map(g).reduction(list, h) `. Dask has no first-party between-invocation caching, but a third-party solution could work. The scheduler would partition the bag into a fixed-number of almost-equal partitions (can be rebalanced by inserting `.repartition(n_partitions=n)`), but the `g` would operate on single-items. `g`'s cache would be reusable. Due to representing batches in the DAG, a batch of `g` can run as soon as its batch of `f`-output is ready. [Reductions](https://docs.dask.org/en/stable/generated/dask.bag.Bag.reduction.html#dask.bag.Bag.reduction) is implemented in 2-levels: `h` on each partition, and `h` on the result of that. The former invocation of `h` is dispatchable as soon as the batch of `g` is done, but the second invocation has to wait for all. If there are a small number of partitions, this is not a big problem, however, this opposes our desire to combat worker imbalance by using a large number of partitions.
- Spark/PySpark: `f().map(g).reduce(h)`. Spark has no first-party between-invocation caching, but a third-party solution could work. `g` operates on single-items. I believe Spark can optimize map-reduce queries, but I couldn't dig enough into the internals easily.
- WDL has [scatter/gather](https://github.com/openwdl/wdl/blob/wdl-1.1/SPEC.md#scatter-statement). Scatter outputs `Array[X]` and gather inputs `Array[X]`, so I think it is unlikely that an implementation would be able to launch streaming jobs. The standard has no mention of cachability, but presumably implementations may implement cachability. `g` inputs a single-element, so presumably the cacheability would work. Presumably, implementations may implement partitioning for more efficient execution.
- The same can be said about CWL as was said about WDL, but with a different link: [scatter/gather](http://www.commonwl.org/user_guide/topics/workflows.html#scattering-steps)
- Cromwell has the limitations inherent to WDL. However, it implements [call caching](https://cromwell.readthedocs.io/en/latest/cromwell_features/CallCaching/). Cromwell schedules [1 job per element](https://cromwell.readthedocs.io/en/latest/developers/bitesize/workflowExecution/executionAndValueStoreExamples/#handling-scatters).
- Toil does not appear to have caching. What they refer to as [caching](https://toil.readthedocs.io/en/latest/appendices/architecture.html#caching) in their documentation refers to within-invocation, same-computational-node. Toil appears to create [1 job per element](https://toil.readthedocs.io/en/latest/appendices/architecture.html#toil-support-for-common-workflow-language).
